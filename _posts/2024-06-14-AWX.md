---
title: AWX 구축
date: 2024-06-14 11:39:00 +09:00
categories: [Ansible, AWX]
tags: [Ansible, AWX] # TAG names should always be lowercase
---

# Ansible

Ansible은 python으로 개발된 IaC 툴입니다. 에이전트 없이 ssh로 통신을 하여 많은 수의 node들을 관리 통합할 수 있습니다.
현재 저희 회사 내부 인프라 관리를 위한 툴로써 Ansible를 사용하고 있습니다. 여러 작업을 동시에 진행하기 위하여 도입되었으며 잘 사용하고 있긴합니다.  
다만, 작업에 대한 이력이나 추가된 playbook등에 대해서 관리가 잘 되지 않고 있습니다.
누가 작업했는지 확인을 위해 log를 따로 찾아서 분석한다거나, 새로운 task를 만들고 각 task를 동작하기 위해서 명령을 수행하는데 반복된 작업에 대해서도 해당 playbook 이나 script등의 위치를 찾아보는 등의 번거롭거나 실수가 발생하기도 했습니다.
이러한 문제들을 해결하고자 했습니다.

# AWX

Ansible을 사용시에 문제점들을 해결할 방법으로 ansible gui 제공되는 툴을 찾게 되었고 Ansible Tower, Ansible Semaphore, Ansible AWX를 알게 되었습니다.

[선택하기 위한 조건]

1.  회사 재정상.. 무료 서비스여야 한다.
2.  관리가 용이하기 위해선 레퍼런스 자료가 많아야 된다.

위 두 조건에 부합한 AWX를 사용하여 관리하기로 하였습니다.

# AWX 구성하기 위한 환경

```bash
1. AWX 환경을 구성하기 위하여 k8s와 ceph 스토리지를 사용합니다.
    - 고가용성을 위함이며, 장기 데이터를 보관하고 따로 스토리지 서버를 구성하지 않고 현재 노드들을 가지고 활용하기 위함입니다.
2. AWX와 Ceph 구성은 helm 차트와 rook 이라는 오픈소스 툴을 사용할 예정입니다.
  # k8s 클러스터 구성에 대해서는 따로 이 글에서 언급하지 않으며, 아래에 링크를 타고 확인해 주시면 됩니다.
3. ceph구성을 위한 볼륨 추가가 필요합니다.
    - 각 node에 ceph osd로 구성할 볼륨을 추가해야 됩니다.(볼륨에 파일시스템을 미리 구성해두면 안됩니다.)
```

# ceph 구성

Ceph는 분산형 스토리지로 여러 스토리지들을 클러스터로 묶어 하나로 보이게하는 스토리지입니다.
장점이라면 분산으로 저장하여 복구에 용이하고 하나의 클러스터로 묶어 클러스터를 바라보게만 하면됩니다.
awx의 데이터들을 관리하기 위한 목적으로 ceph로 분산스토리지를 구성하겠습니다.

rook에서 helm chart도 제공하고 있으나 git에서 직접 operator를 받아서 직접 빌드하겠습니다.

1. host들에 볼륨을 추가합니다.

```bash
worker@controller:~$ lsblk -f
NAME   FSTYPE   FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
loop0  squashfs 4.0                                                    0   100% /snap/lxd/24322
loop1  squashfs 4.0                                                    0   100% /snap/core20/2015
loop2  squashfs 4.0                                                    0   100% /snap/snapd/19457
loop3                                                                  0   100% /snap/lxd/28373
loop4                                                                  0   100% /snap/snapd/21759
loop5                                                                  0   100% /snap/core20/2318
sda
├─sda1
├─sda2 ext4     1.0         4462e5c0-8827-4844-a17b-bd7a6edcf4fc   18.9G    31% /
└─sda3 ext4     1.0         67ec0f67-07c0-4d57-be3b-803b72f089ce   64.8G     0% /home
sdb         # 해당 볼륨이 ceph 구성을 위한 용도입니다.
```

2. rook Operator 배포
   rook-ceph 네임스페이스에 rook-ceph-operator 배포를 합니다.

```bash
git clone --single-branch --branch v1.8.10 https://github.com/rook/rook.git
cd /$PATH/rook/deploy/examples/
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
```

rook배포 후 running 상태 체크

```bash
worker@controller:~/rook/deploy/examples$ kubectl get pod -n rook-ceph
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-6c776cf86c-2qnwz   1/1     Running   0          2m42s
```

3. Ceph Cluster 구성
   cluster 관련하여 다양한 방식의 yaml 파일들이 존재하지만, "cluster.yaml"이 HA를 지원해주는 가장 일반적인 구성파일 입니다.
   해당 파일로 구성을 하겠습니다.

```bash
kubectl create -f cluster.yaml
```

위의 클러스터 설치를 진행하면 버전을 감지하는 pod부터 csi, mon, mgr, osd pod 들이 차례대로 실행됩니다.
전체 구성이 완료되는데는 어느정도 시간이 걸립니다.(k8s controller1, worker2 구성일 경우 문제 없을 시 약 5분정도 경과함)

```bash
worker@controller:~/rook/deploy/examples$ kubectl get pod -A
NAMESPACE          NAME                                            READY   STATUS    RESTARTS   AGE
calico-apiserver   calico-apiserver-67696dcd77-9zjtj               1/1     Running   0          3h17m
calico-apiserver   calico-apiserver-67696dcd77-chldl               1/1     Running   0          3h17m
calico-system      calico-kube-controllers-57bf774f5c-xmw8w        1/1     Running   0          3h19m
calico-system      calico-node-2qhlg                               1/1     Running   0          3h19m
calico-system      calico-node-77wcr                               1/1     Running   0          3h19m
calico-system      calico-node-sx5jw                               1/1     Running   0          3h19m
calico-system      calico-typha-7d9b86c5bd-42pxt                   1/1     Running   0          3h19m
calico-system      calico-typha-7d9b86c5bd-rqq77                   1/1     Running   0          3h19m
calico-system      csi-node-driver-2nvvc                           2/2     Running   0          3h18m
calico-system      csi-node-driver-rzsd6                           2/2     Running   0          3h18m
calico-system      csi-node-driver-w5nk2                           2/2     Running   0          3h18m
kube-system        coredns-7db6d8ff4d-k5jnm                        1/1     Running   0          3d1h
kube-system        coredns-7db6d8ff4d-xwng4                        1/1     Running   0          3d1h
kube-system        etcd-controller                                 1/1     Running   0          3d1h
kube-system        kube-apiserver-controller                       1/1     Running   0          3d1h
kube-system        kube-controller-manager-controller              1/1     Running   0          3d1h
kube-system        kube-proxy-l52pq                                1/1     Running   0          3d1h
kube-system        kube-proxy-q5vkd                                1/1     Running   0          3d1h
kube-system        kube-proxy-x6lf4                                1/1     Running   0          3d1h
kube-system        kube-scheduler-controller                       1/1     Running   0          3d1h
rook-ceph          csi-cephfsplugin-545h6                          2/2     Running   0          18m
rook-ceph          csi-cephfsplugin-fng5c                          2/2     Running   0          18m
rook-ceph          csi-cephfsplugin-provisioner-6b7f5cd57b-l2r2g   5/5     Running   0          18m
rook-ceph          csi-cephfsplugin-provisioner-6b7f5cd57b-zsds7   5/5     Running   0          18m
rook-ceph          csi-rbdplugin-provisioner-66974b7849-fxlht      5/5     Running   0          18m
rook-ceph          csi-rbdplugin-provisioner-66974b7849-jmf82      5/5     Running   0          18m
rook-ceph          csi-rbdplugin-qg4qr                             2/2     Running   0          18m
rook-ceph          csi-rbdplugin-snhzl                             2/2     Running   0          18m
rook-ceph          rook-ceph-mon-a-canary-9bc9c6b7b-49vb2          1/1     Running   0          78s
rook-ceph          rook-ceph-mon-b-canary-94d97dd88-8jxc7          1/1     Running   0          78s
rook-ceph          rook-ceph-mon-c-canary-6876d4cfcb-htr8r         0/1     Pending   0          78s
rook-ceph          rook-ceph-operator-6c776cf86c-2qnwz             1/1     Running   0          27m
rook-ceph          rook-ceph-tools-75f5c94548-7dq8t                1/1     Running   0          6m14s
tigera-operator    tigera-operator-8585875977-q9qpf                1/1     Running   0          3h20m
```

위 결과를 보면 "rook-ceph-mon-c-canary-6876d4cfcb-htr8r" pod가 pending인것을 볼수 있습니다.
확인 시 controller 노드의 taint 설정으로 인하여 발생한 것 입니다.
ceph에서는 분산스토리지 구성을 위해서 3개의 노드를 구성하는것을 권장하고 있습니다.
그렇기 때문에 controller 노드에 untaint 하겠습니다.

```bash
kubectl taint nodes controller node-role.kubernetes.io/control-plane:NoSchedule-
kubectl describe node controller | grep Taints
Taints:             <none>
```

untaint 후 controller노드에도 다른 worker 노드들과 같이 pod들이 구성되는 것을 확인 할 수 있습니다.

4. toolbox 구성
   rook 도구 상자를 구성하여 shell에서 ceph 명령을 사용할 수 있습니다. 또한, ceph 명령으로 스크립트를 실행하고 작업 로그에대한 결과를 수집합니다.

```bash
kubectl create -f deploy/examples/toolbox.yaml
kubectl get pod -A | grep tool
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
```

ceph가 정상적으로 구성되었는지 확인합니다.

```bash
[rook@rook-ceph-tools-75f5c94548-7dq8t /]$ ceph -s
  cluster:
    id:     94e565a5-5be1-468d-b486-9caa792044c3
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 24m)
    mgr: b(active, since 23m), standbys: a
    osd: 3 osds: 3 up (since 23m), 3 in (since 23m)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   15 MiB used, 150 GiB / 150 GiB avail
    pgs:
```

5. 블록 퍼시스턴트 볼륨 구성(RBD)
   Kubernetes에서 Volume이란 컨테이너 내부 또는 컨테이너 간 데이터를 저장하고 공유하기 위한 추상적 개념입니다.
   Kubernetes에서는 볼륨을 공유하기 위한 방식이 여러가지가 존재합니다.
   Persistent Volume(Storage Class)를 이용합니다.
   - Persistent Volume은 Pod의 라이프 사이클과는 상관없이 데이터 관리를 할 수 있는 장점이 있습니다.

```bash

```

toolbox를 통해 확인

```bash
$ kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
$ ceph -s
  cluster:
    id:     94e565a5-5be1-468d-b486-9caa792044c3
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 22h)
    mgr: b(active, since 22h), standbys: a
    osd: 3 osds: 3 up (since 22h), 3 in (since 22h)

  data:
    pools:   2 pools, 33 pgs
    objects: 1 objects, 19 B
    usage:   16 MiB used, 150 GiB / 150 GiB avail
    pgs:     33 active+clean

$ rados lspools
device_health_metrics
replicapool
```

6. Ceph 대시보드 접속
   Ceph를 대시보드로 관리가 가능합니다.

```bash
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
```

7. prometheus 데이터 수집 및 dashboard 구성

```bash
vi rook/deploy/examples/cluster.yaml

  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    # port: 8443
    # serve the dashboard using SSL
    ssl: false                  # 변경 true  ->  false
    urlPrefix: /ceph-dashboard  # 추가
  # enable prometheus alerting for cluster
  monitoring:
    # requires Prometheus to be pre-installed
    enabled: true               # 변경 false  ->  true


# prometheus operator
$ kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml

$ kubectl get pod -A | grep prometheus
default            prometheus-operator-7b9ffd577c-j8p2f                   1/1     Running     0          15m

# prometheus service 구성
worker@controller:~/rook/deploy/examples/monitoring$ kubectl create -f service-monitor.yaml
servicemonitor.monitoring.coreos.com/rook-ceph-mgr created

worker@controller:~/rook/deploy/examples/monitoring$ kubectl create -f prometheus.yaml
serviceaccount/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus-rules created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
prometheus.monitoring.coreos.com/rook-prometheus created

worker@controller:~/rook/deploy/examples/monitoring$ kubectl create -f prometheus-service.yaml
service/rook-prometheus created

# prometheus pod 확인
worker@controller:~/rook/deploy/examples/monitoring$ kubectl -n rook-ceph get pod prometheus-rook-prometheus-0
NAME                           READY   STATUS    RESTARTS      AGE
prometheus-rook-prometheus-0   3/3     Running   1 (79s ago)   99s

```
